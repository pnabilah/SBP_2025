def split_dataset(X, T, train=0.7, val=0.15, test=0.15, seed=None):
    """
    Membagi dataset menjadi train, validation, dan test set.

    Parameters
    ----------
    X : np.ndarray
        Fitur/input dataset.
    T : np.ndarray
        Target/label dataset.
    train, val, test : float
        Proporsi pembagian dataset (jumlahnya harus 1).
    seed : int or None
        Random seed untuk hasil konsisten.

    Returns
    -------
    (X_train, T_train, X_val, T_val, X_test, T_test)
    """
    assert abs(train + val + test - 1.0) < 1e-8, "Proporsi harus berjumlah 1"

    n_total = len(X)
    indices = np.arange(n_total)

    if seed is not None:
        np.random.seed(seed)
    np.random.shuffle(indices)

    n_train = int(train * n_total)
    n_val   = int(val * n_total)

    train_idx = indices[:n_train]
    val_idx   = indices[n_train:n_train+n_val]
    test_idx  = indices[n_train+n_val:]

    return (X[train_idx], T[train_idx],
            X[val_idx],  T[val_idx],
            X[test_idx], T[test_idx])


X_train, T_train, X_val, T_val, X_test, T_test = split_dataset(X, T, 0.7, 0.15, 0.15, seed=42)

print("Train:", X_train.shape, T_train.shape)
print("Val  :", X_val.shape, T_val.shape)
print("Test :", X_test.shape, T_test.shape)


# --- Training loop ---
    def fit(self, X, T, epochs=1000, tol=0.01):
        history = []
        for e in range(epochs):
            total_error = 0
            for i in range(len(X)):
                self.feedforward(X[i])
                error = self.backpropagation(X[i], T[i])
                total_error += error
        return history


class BPNN:
    def __init__(self, input_size, hidden_size, output_size, lr=0.001, seed=42):
        np.random.seed(seed)

        # struktur
        self.NI = input_size
        self.NH = hidden_size
        self.NO = output_size
        self.lr = lr

        # inisialisasi bobot
        self.v = np.random.rand(self.NI, self.NH)   # input â†’ hidden
        self.vb = np.random.rand(self.NH)           # bias hidden
        self.w = np.random.rand(self.NH, self.NO)   # hidden â†’ output
        self.wb = np.random.rand(self.NO)           # bias output

    # --- Activation functions ---
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoid_derivative(self, x):
        s = self.sigmoid(x)
        return s * (1 - s)

    # --- Feedforward ---
    def feedforward(self, x):
        # hidden
        self.z_in = np.dot(x, self.v) + self.vb
        self.z = self.sigmoid(self.z_in)

        # output (pakai linear, bisa diganti sigmoid/tanh kalau mau)
        self.y_in = np.dot(self.z, self.w) + self.wb
        self.y = self.y_in  

        return self.y

    # --- Backpropagation ---
    def backpropagation(self, x, t):
        # error output
        delta_y = (t - self.y)
        del_w = self.lr * np.outer(self.z, delta_y)
        del_wb = self.lr * delta_y

        # error hidden
        delta_zin = np.dot(delta_y, self.w.T)
        delta_z = delta_zin * self.z * (1 - self.z)  # sigmoid derivative
        del_v = self.lr * np.outer(x, delta_z)
        del_vb = self.lr * delta_z

        # update bobot
        self.w += del_w
        self.wb += del_wb
        self.v += del_v
        self.vb += del_vb

        return np.mean(delta_y**2)   # return MSE

    # --- Training loop with validation ---
    def fit(self, X_train, T_train, X_val=None, T_val=None,
            epochs=1000, tol=0.01, patience=20):
        history = {"train_loss": [], "val_loss": []}
        best_val_loss = float("inf")
        patience_ctr = 0

        for epoch in range(epochs):
            mse = 0
            # training
            for x, t in zip(X_train, T_train):
                self.feedforward(x)
                mse += self.backpropagation(x, t)
            mse /= len(X_train)
            history["train_loss"].append(mse)

            # validation (no weight update)
            if X_val is not None and T_val is not None:
                val_loss = 0
                for x, t in zip(X_val, T_val):
                    y_pred = self.feedforward(x)
                    val_loss += np.mean((t - y_pred)**2)
                val_loss /= len(X_val)
                history["val_loss"].append(val_loss)

                # early stopping check
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_ctr = 0
                else:
                    patience_ctr += 1
                    if patience_ctr >= patience:
                        print(f"Early stopping at epoch {epoch}, val_loss={val_loss:.6f}")
                        break
            # tolerance check (on training loss)
            if mse < tol:
                print(f"Training stopped at epoch {epoch}, train_loss={mse:.6f}")
                break

        return history

    # --- Prediction ---
    def predict(self, X):
        outputs = []
        for x in X:
            outputs.append(self.feedforward(x))
        return np.array(outputs)
